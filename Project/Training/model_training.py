# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17g_iFbiN8BKe0chLtODVZ0bGZ0dzwwnP
"""

#imports
import zipfile
import os
import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle as shuffle_dataframe
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# From sensorlogger generated csv to pandas dataframe for training/testing ADL instance generation

def butter_lowpass_filter(data, cutoff, fs, order=4):
    nyquist = 0.5 * fs  # Nyquist frequency
    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency
    b, a = butter(order, normal_cutoff, btype='low', analog=False)  # Filter coefficients
    y = filtfilt(b, a, data)
    return y

def sensorLoggerCSVtoADLDatasetDF(sensorLoggerCSV, hardCodedLabel=0):
    pd_accel = pd.read_csv(sensorLoggerCSV)
    magList = np.sqrt(pd_accel["x"]**2 + pd_accel["y"]**2 + pd_accel["z"]**2)
    cutoff_frequency = 5  # 5 hz cutoff frequency
    sampling_frequency = 100  # Sampling frequency (set to same in SensorLogger app)
    filtered_magnitude = butter_lowpass_filter(magList, cutoff_frequency, sampling_frequency)
    magList = filtered_magnitude
    newDF = pd.DataFrame(columns=['magnitude', 'label'])
    for i in range(int(np.floor(len(magList)/350)) -1): # Mimic the overlapping buffers at server side
        new_row = pd.DataFrame([{'label': hardCodedLabel, 'magnitude': magList[i*350:(i+2)*350]}])
        newDF = pd.concat([newDF, new_row], ignore_index=True)
    return newDF

# collective dataframe's adl data

# ADL csv #1
df_all_adl1 = sensorLoggerCSVtoADLDatasetDF("ADHL.csv", 0)
# ADL csv #2
df_all_adl2 = sensorLoggerCSVtoADLDatasetDF("fenste_cesitli_adl.csv", 0)

# grouped
df_all_adl = pd.concat([df_all_adl1, df_all_adl2], ignore_index=True)

# Check data format
print(df_all_adl.head()) # magnitude and label, length = 677
print(df_all_adl.shape, len(df_all_adl['magnitude'][17])) # randomly selected 17th instance for checking

#Check types, should be numpy array

#print(type(df_all_adl['magnitude'][17])) # type = numpy.ndarray
#print(type(df_all_adl['magnitude'][17][24])) # type = numpy.float64

## From sensorlogger generated csv to pandas dataframe for training/testing Fall instance generation

def butter_lowpass_filter(data, cutoff, fs, order=4):
    nyquist = 0.5 * fs  # Nyquist frequency
    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency
    b, a = butter(order, normal_cutoff, btype='low', analog=False)  # Filter coefficients
    y = filtfilt(b, a, data)
    return y

def ZiptoSensorLoggerCSVtoFallDatasetDF(zip_file_path, label):
    """
    Parse CSV files in a zip archive, compute filtered magnitudes, and return a DataFrame.

    Parameters:
    - zip_file_path: Path to the zip file containing CSV files.
    - label: A label to assign to each processed row in the output DataFrame.

    Returns:
    - A DataFrame with columns ['magnitude', 'label'].
    """
    cutoff_frequency = 5
    sampling_frequency = 100 # same in Sensor Logger app
    final_df = pd.DataFrame(columns=["magnitude", "label"])

    with zipfile.ZipFile(zip_file_path, 'r') as zf:
        # Iterate over each file in the zip
        for file_name in zf.namelist():
            if file_name.endswith('.csv'):  # only process CSV files
                with zf.open(file_name) as f:
                    df = pd.read_csv(f)
                    # Compute magnitude
                    mag_list = np.sqrt(df["x"][:700]**2 + df["y"][:700]**2 + df["z"][:700]**2)

                    if len(mag_list) != 700:  # some frames may not have arrived as 700, but longer
                        print(f"Skipping file {file_name} due to length mismatch.")
                        continue

                    filtered_magnitude = butter_lowpass_filter(mag_list,
                                                               cutoff_frequency,
                                                               sampling_frequency)

                    # Ensure magnitude is stored as a NumPy array of np.float64
                    filtered_magnitude = np.array(filtered_magnitude, dtype=np.float64)

                    # Add a row to the final DataFrame
                    final_df = pd.concat([final_df, pd.DataFrame({
                        "magnitude": [filtered_magnitude],  # Store as ndarray
                        "label": [label]
                    })], ignore_index=True)

    return final_df

# collective dataframe's fall data
df_all_fall = ZiptoSensorLoggerCSVtoFallDatasetDF("fall.zip", 1) # 1 is the class of Fall

# # Check data format
print(df_all_fall.head()) # magnitude and label, length = 252
print(df_all_fall.shape, len(df_all_fall['magnitude'][17])) # randomly selected 17 for checking

# Check the types, should be numpy array
#print(type(df_all_fall['magnitude'][17])) # type = numpy.ndarray
#print(type(df_all_fall['magnitude'][17][24])) # type = numpy.float64

"""

---


in total, we have

* 677 non-fall frames

* 252 fall frames


---

"""

# partition into train(75%) and test(25%), and save each as csv for checkpoint saving

def split_and_concatenate(df_0, df_1, split_ratio=0.8, random_state=None, shuffle=True):
    """
    Splits two dataframes into train and test sets, concatenates the respective train and test sets,
    and returns the concatenated dataframes.

    Parameters:
    - df_0, df_1: DataFrames with "magnitude" and "label" columns.
    - split_ratio: Fraction of data to use for training (default is 0.8).
    - random_state: Seed for reproducibility (default is None).
    - shuffle: Whether to shuffle the data before splitting (default is True).

    Returns:
    - concatenated_train: DataFrame containing the concatenated training data.
    - concatenated_test: DataFrame containing the concatenated testing data.
    """
    # Split dataframe_0
    train_0, test_0 = train_test_split(df_0, train_size=split_ratio, random_state=random_state, shuffle=shuffle)

    # Split dataframe_1
    train_1, test_1 = train_test_split(df_1, train_size=split_ratio, random_state=random_state, shuffle=shuffle)

    # Concatenate the train and test sets from both dataframes
    concatenated_train = pd.concat([train_0, train_1], ignore_index=True)
    concatenated_test = pd.concat([test_0, test_1], ignore_index=True)

    # this way, ratio of fall in the training and testing is preserved
    concatenated_train = shuffle_dataframe(concatenated_train, random_state=random_state).reset_index(drop=True)

    return concatenated_train, concatenated_test


df_train, df_test = split_and_concatenate(df_all_fall, df_all_adl, split_ratio=0.75, random_state=42)

# check the dfs
#print(df_train) # 696 rows
#print(df_test) # 233 rows

# save the dataframes before training for easier access next time
df_train.to_csv('df_train.csv', index=False)
df_test.to_csv('df_test.csv', index=False)

#print(type(df_train["magnitude"][42])) # <class 'numpy.ndarray'>
#print(type(df_train["magnitude"][42][42])) # <class 'numpy.float64'>

#print(type(df_test["magnitude"][42])) # <class 'numpy.ndarray'>
#print(type(df_test["magnitude"][42][42])) # <class 'numpy.float64'>

# Load the datasets
#df_train = pd.read_csv("df_train.csv")
#df_test = pd.read_csv("df_test.csv")

import matplotlib.pyplot as plt

def create_bar_chart(label1, label2, count1, count2):
    """
    Creates a bar chart with two bins based on given labels and counts.

    Args:
        label1 (str): Label for the first bin.
        label2 (str): Label for the second bin.
        count1 (int or float): Count for the first bin.
        count2 (int or float): Count for the second bin.
    """
    labels = [label1, label2]
    counts = [count1, count2]

    plt.figure(figsize=(6, 4))
    plt.bar(labels, counts, color=['blue', 'orange'])
    plt.xlabel('Categories')
    plt.ylabel('Counts')
    plt.title('Distribution of Classes')
    plt.show()


create_bar_chart("Fall instances", "non-Fall instances", 252, 677)

# construct the GRU model

model = Sequential([
    GRU(128, input_shape=(700, 1), return_sequences=True),  # GRU layer with 128 units
    Dropout(0.3),  # Dropout to prevent overfitting
    GRU(64, return_sequences=False),  # GRU layer with 64 units
    Dropout(0.3),  # Dropout for regularization
    Dense(32, activation='relu'),  # Fully connected layer
    Dropout(0.2),  # Dropout before output layer
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(model.summary())

# get ready for training

def prepare_training_testing_data(df_train, df_test):
    """
    Prepares training and testing data by separating features (X) and labels (Y).

    Parameters:
        df_train (pd.DataFrame): Training DataFrame with 'magnitude' and 'label' columns.
        df_test (pd.DataFrame): Testing DataFrame with 'magnitude' and 'label' columns.

    Returns:
        x_train, y_train: Features and labels for training.
        x_test, y_test: Features and labels for testing.
    """
    # Extract features (X) and labels (Y) for training data
    x_train = np.stack(df_train["magnitude"].values)  # Stack magnitudes into a 2D NumPy array
    y_train = df_train["label"].values.astype(np.float32)  # Convert labels to a NumPy array of float32

    # Extract features (X) and labels (Y) for testing data
    x_test = np.stack(df_test["magnitude"].values)  # Stack magnitudes into a 2D NumPy array
    y_test = df_test["label"].values.astype(np.float32)  # Convert labels to a NumPy array of float32

    return x_train, y_train, x_test, y_test


# Example usage:
x_train, y_train, x_test, y_test = prepare_training_testing_data(df_train, df_test)

print("x_train shape:", x_train.shape)  # e.g., (num_samples_train, 700)
print("y_train shape:", y_train.shape)  # e.g., (num_samples_train,)

print("x_test shape:", x_test.shape)
print("y_test shape:", y_test.shape)

# save splitted ndarrays for easier access
np.save('x_train.npy', x_train)
np.save('y_train.npy', y_train)
np.save('x_test.npy', x_test)
np.save('y_test.npy', y_test)

# load splitted ndarrays for easier access to do gpu training at the next step
#x_train = np.load('x_train.npy', allow_pickle=True)
#y_train = np.load('y_train.npy', allow_pickle=True)
#x_test = np.load('x_test.npy', allow_pickle=True)
#y_test = np.load('y_test.npy', allow_pickle=True)

# training (run imports, model architecture and np.loads if running this on gpu)

log_dir = os.path.join("logs", "fit", "run_1")  # Change "run_1" for different training runs
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)

history = model.fit(
    x_train,
    y_train,
    verbose=1,
    epochs=200,  # Number of epochs can be 20
    batch_size=32,
    validation_split=0.2,  # Use 20% of training data for validation
    callbacks=[tensorboard_callback]
)

# Save the trained model
model.save("my_gru_model.weights.h5")  # Save the model in HDF5 format

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")