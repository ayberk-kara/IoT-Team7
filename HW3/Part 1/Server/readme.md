This Flask server detects number of people people in uploaded images using a YOLO model. It can run on GPU if available, otherwise on CPU. The result is broadcasted on GET endpoint.

The specified flavor of YOLO model has been observed to be running the inference in around 100ms in the tested hardware setup with CPU based execution. Besides this computation, two other factors contribute to the overall period of the operation: Client operation and network transmission of a frame. To account for them, Client names the frame with the timestamp of its capture. Once this frame is received at the server side, name is read to a variable that is then subtracted from the current time to obtain the result for the overall operation. To have this work, time functions executed on different platforms should be in sync. According to the observations, taken total time is dominated by the computation of inference on the server side. This situation is also observed with different flavours of YOLO model, increasing with model size. As a result, printed "Total time" metric can be used as the overall length of the interval of operation if synchronization between the client and server is not established.

As mentioned, inference computation bounds the operation to 10 FPS, given a single instance takes 100ms. Different hardware can report lower/higher inference timings, specially utilization of a GPU. However, network speed can be the bottleneck on the whole operation as well. In that case, synchronization must be established to measure the network transmission time, and printed "Time passed in ms" metric should be used as the timing result.

Important Configuration:
The server is meant to run in virtual envirionment such as a docker container or a VM and will listen on port 8000. Therefore the port forwarding should be set 80 (host machine) -> 8000 (virtual).
