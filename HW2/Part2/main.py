# -*- coding: utf-8 -*-

"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zZ5N4Rdw-ApEH6C5DCOXRe3SK4Y0uE_t

## Import Necessary Libraries
"""
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from sklearn.metrics import classification_report
from math import ceil
from sklearn.utils import shuffle
import zipfile

"""## Arranging Data"""

# Define the path to the zip file
zip_path = input("Please enter the path to the UCI HAR Dataset zip file: ")

# Open the zip file .\test_hw2\UCI HAR Dataset.zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    # Define paths within the zip for test and train files
    subject_test_path = 'UCI HAR Dataset/test/subject_test.txt'
    x_test_path = 'UCI HAR Dataset/test/X_test.txt'
    y_test_path = 'UCI HAR Dataset/test/y_test.txt'

    inertial_test_files = {
        'body_acc_x_test': 'UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt',
        'body_acc_y_test': 'UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt',
        'body_acc_z_test': 'UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt',
        'body_gyro_x_test': 'UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt',
        'body_gyro_y_test': 'UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt',
        'body_gyro_z_test': 'UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt',
        'total_acc_x_test': 'UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt',
        'total_acc_y_test': 'UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt',
        'total_acc_z_test': 'UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt'
    }

    subject_train_path = 'UCI HAR Dataset/train/subject_train.txt'
    x_train_path = 'UCI HAR Dataset/train/X_train.txt'
    y_train_path = 'UCI HAR Dataset/train/y_train.txt'

    inertial_train_files = {
        'body_acc_x_train': 'UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt',
        'body_acc_y_train': 'UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt',
        'body_acc_z_train': 'UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt',
        'body_gyro_x_train': 'UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt',
        'body_gyro_y_train': 'UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt',
        'body_gyro_z_train': 'UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt',
        'total_acc_x_train': 'UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt',
        'total_acc_y_train': 'UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt',
        'total_acc_z_train': 'UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt'
    }

    with zip_ref.open(subject_test_path) as f:
        subject_test = np.loadtxt(f, dtype=int)
    with zip_ref.open(x_test_path) as f:
        x_test = np.loadtxt(f)
    with zip_ref.open(y_test_path) as f:
        y_test = np.loadtxt(f, dtype=int)

    with zip_ref.open(subject_train_path) as f:
        subject_train = np.loadtxt(f, dtype=int)
    with zip_ref.open(x_train_path) as f:
        x_train = np.loadtxt(f)
    with zip_ref.open(y_train_path) as f:
        y_train = np.loadtxt(f, dtype=int)

    body_acc_x_test = np.loadtxt(zip_ref.open(inertial_test_files['body_acc_x_test'])).astype(np.float32)
    body_acc_y_test = np.loadtxt(zip_ref.open(inertial_test_files['body_acc_y_test'])).astype(np.float32)
    body_acc_z_test = np.loadtxt(zip_ref.open(inertial_test_files['body_acc_z_test'])).astype(np.float32)
    body_gyro_x_test = np.loadtxt(zip_ref.open(inertial_test_files['body_gyro_x_test'])).astype(np.float32)
    body_gyro_y_test = np.loadtxt(zip_ref.open(inertial_test_files['body_gyro_y_test'])).astype(np.float32)
    body_gyro_z_test = np.loadtxt(zip_ref.open(inertial_test_files['body_gyro_z_test'])).astype(np.float32)
    total_acc_x_test = np.loadtxt(zip_ref.open(inertial_test_files['total_acc_x_test'])).astype(np.float32)
    total_acc_y_test = np.loadtxt(zip_ref.open(inertial_test_files['total_acc_y_test'])).astype(np.float32)
    total_acc_z_test = np.loadtxt(zip_ref.open(inertial_test_files['total_acc_z_test'])).astype(np.float32)

    body_acc_x_train = np.loadtxt(zip_ref.open(inertial_train_files['body_acc_x_train'])).astype(np.float32)
    body_acc_y_train = np.loadtxt(zip_ref.open(inertial_train_files['body_acc_y_train'])).astype(np.float32)
    body_acc_z_train = np.loadtxt(zip_ref.open(inertial_train_files['body_acc_z_train'])).astype(np.float32)
    body_gyro_x_train = np.loadtxt(zip_ref.open(inertial_train_files['body_gyro_x_train'])).astype(np.float32)
    body_gyro_y_train = np.loadtxt(zip_ref.open(inertial_train_files['body_gyro_y_train'])).astype(np.float32)
    body_gyro_z_train = np.loadtxt(zip_ref.open(inertial_train_files['body_gyro_z_train'])).astype(np.float32)
    total_acc_x_train = np.loadtxt(zip_ref.open(inertial_train_files['total_acc_x_train'])).astype(np.float32)
    total_acc_y_train = np.loadtxt(zip_ref.open(inertial_train_files['total_acc_y_train'])).astype(np.float32)
    total_acc_z_train = np.loadtxt(zip_ref.open(inertial_train_files['total_acc_z_train'])).astype(np.float32)

    body_acc_x = np.concatenate((body_acc_x_train, body_acc_x_test), axis=0)
    body_acc_y = np.concatenate((body_acc_y_train, body_acc_y_test), axis=0)
    body_acc_z = np.concatenate((body_acc_z_train, body_acc_z_test), axis=0)
    body_gyro_x = np.concatenate((body_gyro_x_train, body_gyro_x_test), axis=0)
    body_gyro_y = np.concatenate((body_gyro_y_train, body_gyro_y_test), axis=0)
    body_gyro_z = np.concatenate((body_gyro_z_train, body_gyro_z_test), axis=0)
    total_acc_x = np.concatenate((total_acc_x_train, total_acc_x_test), axis=0)
    total_acc_y = np.concatenate((total_acc_y_train, total_acc_y_test), axis=0)
    total_acc_z = np.concatenate((total_acc_z_train, total_acc_z_test), axis=0)

    subjects_concatenated = np.concatenate((subject_train, subject_test), axis=0)
    y_concatenated = np.concatenate((y_train, y_test), axis=0)
    x_concatenated = np.concatenate((x_train, x_test), axis=0)

# Define the CNN model
# CNN used in other sections
def create_cnn_model(input_shape=(128, 9), num_classes=6):
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(shape=input_shape),
        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(100, activation='relu'),
        tf.keras.layers.Dropout(0.5),  # Dropout for regularization
        tf.keras.layers.Dense(50, activation='relu'),
        tf.keras.layers.BatchNormalization(),  # Batch Normalization for stable training
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model




# CNN used for C) Training + Testing (Custom Set - Personal)
def create_cnn_model_1(input_shape=(128, 9), num_classes=6):
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(shape=input_shape),
        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(100, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(50, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model

# Uses training data
# A1) Training (Default Set - Collective) parse_data function
def parse_sensor_data():
    frames = np.zeros((7352, 128, 11))
    frames[:, :, 0] = body_acc_x_train
    frames[:, :, 1] = body_acc_y_train
    frames[:, :, 2] = body_acc_z_train
    frames[:, :, 3] = body_gyro_x_train
    frames[:, :, 4] = body_gyro_y_train
    frames[:, :, 5] = body_gyro_z_train
    frames[:, :, 6] = total_acc_x_train
    frames[:, :, 7] = total_acc_y_train
    frames[:, :, 8] = total_acc_z_train
    frames[:, :, 9] = y_train[:, None]
    frames[:, :, 10] = subject_train[:, None]

    return frames


# Uses testing data
# A2) Testing (Default Set - Collective) parse_data
def parse_test_data():
    # Initialize the frames array
    num_frames = len(body_acc_x_test)
    frames = np.zeros((num_frames, 128, 11))  # Assuming 2947 test samples

    # Populate the frames directly
    frames[:, :, 0] = body_acc_x_test
    frames[:, :, 1] = body_acc_y_test
    frames[:, :, 2] = body_acc_z_test
    frames[:, :, 3] = body_gyro_x_test
    frames[:, :, 4] = body_gyro_y_test
    frames[:, :, 5] = body_gyro_z_test
    frames[:, :, 6] = total_acc_x_test
    frames[:, :, 7] = total_acc_y_test
    frames[:, :, 8] = total_acc_z_test
    frames[:, :, 9] = y_test[:, None]
    frames[:, :, 10] = subject_test[:, None]

    return frames



# Uses combined data
# B) Training + Testing (Custom Set - Collective)
# C) Training + Testing (Custom Set - Personal)
# D) Training + Testing (Collective model - Transferred to Personal)
def parse_combined_sensor_data():

    # Initialize frames with all data
    frames = np.zeros((7352 + 2947, 128, 11))
    frames[:, :, 0] = body_acc_x
    frames[:, :, 1] = body_acc_y
    frames[:, :, 2] = body_acc_z
    frames[:, :, 3] = body_gyro_x
    frames[:, :, 4] = body_gyro_y
    frames[:, :, 5] = body_gyro_z
    frames[:, :, 6] = total_acc_x
    frames[:, :, 7] = total_acc_y
    frames[:, :, 8] = total_acc_z
    frames[:, :, 9] = y_concatenated[:, None]
    frames[:, :, 10] = subjects_concatenated[:, None]
    return frames

"""## A1) Training (Default Set - Collective)"""

print("\n------------------------------------------------------")
print("A1) Training (Default Set - Collective)\n")
frames = parse_sensor_data()
X = frames[:, :, :9]
y = frames[:, 0, 9].astype(int) - 1
# Shuffle data
indices = np.arange(len(X))
np.random.shuffle(indices)
X = X[indices]
y = y[indices]
# Define the CNN model
model = create_cnn_model()

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Callbacks for training
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)

# Train the model
history = model.fit(X, y, epochs=20, batch_size=64, validation_split=0.2,
                    callbacks=[early_stopping, model_checkpoint])

print("\n------------------------------------------------------")

"""## A2) Testing (Default Set - Collective)"""

print("\n------------------------------------------------------")
print("A2) Testing (Default Set - Collective)\n")

# Load and prepare test data
test_frames = parse_test_data()
X_test = test_frames[:, :, :9]  # Features (first 6 columns)
y_test = test_frames[:, 0, 9].astype(int) - 1  # Adjust labels

# Sanity check on shapes
#print("Shape of X_test:", X_test.shape)  # Expected: (num_test_samples, 128, 9)
#print("Shape of y_test:", y_test.shape)  # Expected: (num_test_samples,)

# Load the best model (if you saved it during training)
model = tf.keras.models.load_model('best_model.keras')

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')

# If needed, make predictions on the test data
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
y_pred = predicted_classes
report = classification_report(y_test, y_pred,output_dict=True ,target_names=[f'Class {i}' for i in range(6)], zero_division=0)

# Extract and print weighted average metrics
weighted_precision = report['weighted avg']['precision']
weighted_recall = report['weighted avg']['recall']
weighted_f1 = report['weighted avg']['f1-score']

print(f"Weighted Precision: {weighted_precision:.4f}")
print(f"Weighted Recall: {weighted_recall:.4f}")
print(f"Weighted F1-Score: {weighted_f1:.4f}")

# Compare predictions with true labels
correct_predictions = np.sum(predicted_classes == y_test)
print(f'Correct Predictions: {correct_predictions}/{len(y_test)}')
print("\n------------------------------------------------------")

"""## B) Training + Testing (Custom Set - Collective)"""

print("\n------------------------------------------------------")
print("B) Training + Testing (Custom Set - Collective)\n")
# Load and parse combined data
frames = parse_combined_sensor_data()

#shuffle frame order(labels are not dropped yet)
frame_indices = np.arange(len(frames))
np.random.shuffle(frame_indices)
frames = frames[frame_indices]


# Splitting data by subject
unique_subjects = np.unique(frames[:, 0, 10])
train_frames = []
test_frames = []
#print("Combined set consists of data from the following subjectIDs:", unique_subjects)
for subject_id in unique_subjects:
    subject_frames = frames[frames[:, 0, 10] == subject_id]
    num_frames = subject_frames.shape[0]

    # Calculate number of frames for training and testing
    num_train = ceil(num_frames * 0.75)  # 75% for training
    num_test = num_frames - num_train  # Remaining for testing

    # Split frames for this subject
    train_frames.append(subject_frames[:num_train])
    test_frames.append(subject_frames[num_train:])

# Concatenate all training and testing frames
train_frames = np.concatenate(train_frames)
test_frames = np.concatenate(test_frames)

# Prepare X (features) and y (labels) for training and testing
X_train = train_frames[:, :, :9]
y_train = train_frames[:, 0, 9].astype(int) - 1  # Shift labels to range [0, 5]
X_test = test_frames[:, :, :9]
y_test = test_frames[:, 0, 9].astype(int) - 1

# Instantiate and compile the model
model = create_cnn_model()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Callbacks for training
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
model_checkpoint = ModelCheckpoint('best_customSet_model.keras', save_best_only=True)

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2,
                    callbacks=[early_stopping, model_checkpoint])

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
# Get model predictions for the test set
y_pred = model.predict(X_test).argmax(axis=1)  # Convert predictions to class indices

report = classification_report(y_test, y_pred, output_dict=True, target_names=[f'Class {i}' for i in range(6)], zero_division=0)


# Extract and print weighted average metrics
weighted_precision = report['weighted avg']['precision']
weighted_recall = report['weighted avg']['recall']
weighted_f1 = report['weighted avg']['f1-score']

print(f"Weighted Precision: {weighted_precision:.4f}")
print(f"Weighted Recall: {weighted_recall:.4f}")
print(f"Weighted F1-Score: {weighted_f1:.4f}")

# Compare predictions with true labels
correct_predictions = np.sum(y_pred == y_test)
print(f'Correct Predictions: {correct_predictions}/{len(y_test)}')
print("\n------------------------------------------------------")

"""## C) Training + Testing (Custom Set - Personal)"""

print("\n------------------------------------------------------")
print("C) Training + Testing (Custom Set - Personal)\n")

# Load and parse combined data
frames = parse_combined_sensor_data()
frame_indices = np.arange(len(frames))
np.random.shuffle(frame_indices)
frames = frames[frame_indices]

unique_subjects = np.unique(frames[:, 0, 10])

# Dictionary to hold personalized models
models_personal = {}
accuracies = []
precision_scores=[]
recall_scores=[]
f1_scores = []


# Loop over each subject
for subject_id in unique_subjects:
    # Filter frames for the current subject
    subject_frames = frames[frames[:, 0, 10] == subject_id]
    num_frames = subject_frames.shape[0]

    # Determine train/test split counts
    num_train = ceil(num_frames * 0.8)
    num_test = num_frames - num_train

    # Split frames into training and testing sets
    train_frames = subject_frames[:num_train]
    test_frames = subject_frames[num_train:]

    # Prepare X and y for the subject
    X_train = train_frames[:, :, :9]
    y_train = train_frames[:, 0, 9].astype(int) - 1
    X_test = test_frames[:, :, :9]
    y_test = test_frames[:, 0, 9].astype(int) - 1

    # Instantiate and compile the model
    model = create_cnn_model_1()
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Callbacks for training
    early_stopping = EarlyStopping(monitor='val_loss', patience=5)
    #model_checkpoint = ModelCheckpoint(f'best_model_subject_{int(subject_id)}.keras', save_best_only=True)

    # Train the model for the current subject
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2,
              callbacks=[early_stopping], verbose=0)

    # Evaluate the model on the test set
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    y_pred = model.predict(X_test, verbose=0).argmax(axis=1)  # Convert predictions to class indices
    unique_classes = np.unique(y_test)
    #print(unique_classes)
    # Generate dynamic target names based on unique classes
    target_names = [f'Class {i}' for i in unique_classes]

    report = classification_report(y_test, y_pred, labels=unique_classes, output_dict=True, target_names=target_names, zero_division=0)

    # Extract and print weighted average metrics
    weighted_precision = report['weighted avg']['precision']
    weighted_recall = report['weighted avg']['recall']
    weighted_f1 = report['weighted avg']['f1-score']
    accuracies.append(test_accuracy)
    precision_scores.append(weighted_precision)
    recall_scores.append(weighted_recall)
    f1_scores.append(weighted_f1)
    # Print subject details
    print(f"Subject {int(subject_id)} - Train Instances: {num_train}, Test Instances: {num_test}, Test Accuracy: {test_accuracy:.4f}",
          f"|| Weighted Precision: {weighted_precision:.4f}, Weighted Recall: {weighted_recall:.4f}, Weighted F1-Score: {weighted_f1:.4f}")

    # Store the trained model in the dictionary
    models_personal[subject_id] = model

# Calculate and print the average accuracy
average_accuracy = np.mean(accuracies)
avg_precision_scores = np.mean(precision_scores)
avg_recall_scores = np.mean(recall_scores)
avg_f1_scores = np.mean(f1_scores)
print(f"\nAverage Accuracy across all subjects: {average_accuracy:.4f}",
      f"|| Weighted Precision: {avg_precision_scores:.4f}, Weighted Recall: {avg_recall_scores:.4f}, Weighted F1-Score: {avg_f1_scores:.4f}")
print("\n------------------------------------------------------")

"""## D) Training + Testing (Collective model - Transferred to Personal)"""

print("\n------------------------------------------------------")
print("D) Training + Testing (Collective model - Transferred to Personal)\n")

# Load and parse combined data
frames = parse_combined_sensor_data()
frame_indices = np.arange(len(frames))
np.random.shuffle(frame_indices)
frames = frames[frame_indices]
unique_subjects = np.unique(frames[:, 0, 10])

# Temporary storage for training and test sets
subject_data = []

# Step 1: Split data for each subject and shuffle
for subject_id in unique_subjects:
    # Filter frames for the current subject
    subject_frames = frames[frames[:, 0, 10] == subject_id]
    num_frames = subject_frames.shape[0]

    # Determine train/test split counts
    num_train = ceil(num_frames * 0.8)
    num_test = num_frames - num_train

    # Shuffle the frames
    subject_frames = shuffle(subject_frames)

    # Split frames into training and testing sets
    train_frames = subject_frames[:num_train]
    test_frames = subject_frames[num_train:]

    # Store training and test data
    subject_data.append({
        'subject_id': subject_id,
        'train_frames': train_frames,
        'test_frames': test_frames,
    })

# Concatenate all training data for general model training
X_train_general = np.concatenate([data['train_frames'][:, :, :9] for data in subject_data])
y_train_general = np.concatenate([data['train_frames'][:, 0, 9].astype(int) - 1 for data in subject_data])

# Step 2: Train a general model
general_model = create_cnn_model()
general_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Callbacks for saving the best model
model_checkpoint = ModelCheckpoint('best_generalCustomSet_model.keras', save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', patience=5)

# Train the model with the concatenated training data
general_model.fit(X_train_general, y_train_general, epochs=20, batch_size=32, validation_split=0.2,
                  callbacks=[model_checkpoint, early_stopping], verbose=0)

# Step 3: Transfer Learning
transfer_accuracies = []
transfer_precision_scores=[]
transfer_recall_scores=[]
transfer_f1_scores = []

# Loop for each subject
for i in range(len(subject_data)):
    data = subject_data[i]
    subject_id = data['subject_id']
    train_frames = data['train_frames']
    test_frames = data['test_frames']

    # Prepare training data for transfer learning
    X_transfer = train_frames[:, :, :9]
    y_transfer = train_frames[:, 0, 9].astype(int) - 1

    # Load the best general model for transfer learning
    transfer_model = create_cnn_model()
    transfer_model.load_weights('best_generalCustomSet_model.keras')

    # Freeze the last 4 layers
    for layer in transfer_model.layers[-4:]:
        layer.trainable = False

    # Compile the model
    transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Fine-tune the model on the transfer data
    transfer_model.fit(X_transfer, y_transfer, epochs=20, batch_size=16, validation_split=0.2,
                       callbacks=[early_stopping], verbose=0)

    # Test the model with the corresponding test data
    X_test = test_frames[:, :, :9]
    y_test = test_frames[:, 0, 9].astype(int) - 1
    test_loss, test_accuracy = transfer_model.evaluate(X_test, y_test, verbose=0)

    y_pred = transfer_model.predict(X_test,verbose=0).argmax(axis=1)  # Convert predictions to class indices
    transfer_accuracies.append(test_accuracy)
    unique_classes = np.unique(y_test)

    # Generate dynamic target names based on unique classes
    target_names = [f'Class {i}' for i in unique_classes]

    # Extract and print weighted average metrics
    report = classification_report(y_test, y_pred,  labels=unique_classes,output_dict=True, target_names=target_names, zero_division=0)
    weighted_precision = report['weighted avg']['precision']
    weighted_recall = report['weighted avg']['recall']
    weighted_f1 = report['weighted avg']['f1-score']
    # Print subject details for transfer learning
    print(f"Subject {int(subject_id)} - Transfer Test Accuracy: {test_accuracy:.4f}",
          f"|| Weighted Precision: {weighted_precision:.4f}, Weighted Recall: {weighted_recall:.4f}, Weighted F1-Score: {weighted_f1:.4f}")
    transfer_precision_scores.append(weighted_precision)
    transfer_recall_scores.append(weighted_recall)
    transfer_f1_scores.append(weighted_f1)


# Average accuracy after transfer learning
average_transfer_accuracy = np.mean(transfer_accuracies)
average_transfer_precision_scores = np.mean(transfer_precision_scores)
average_transfer_recall_scores = np.mean(transfer_recall_scores)
average_transfer_f1_scores = np.mean(transfer_f1_scores)
print(f"\n(After transfer learning) Average Accuracy across all subjects : {average_transfer_accuracy:.4f}",
      f"|| Weighted Precision: {average_transfer_precision_scores:.4f}, Weighted Recall: {average_transfer_recall_scores:.4f}, Weighted F1-Score: {average_transfer_f1_scores:.4f}")
print("\n------------------------------------------------------")
